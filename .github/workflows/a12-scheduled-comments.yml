name: "A12: scheduled comments & PR"

on:
  workflow_dispatch:
  schedule:
    - cron: "43 7 * * *"   # 任意。お好みで調整OK

jobs:
  scrape-and-pr:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install -U pip
          pip install -U requests beautifulsoup4 lxml fastapi pydantic-settings pandas

      - name: Prepare meta (UTC)
        run: |
          echo "UTC_DATE=$(date -u +%Y-%m-%d)" >> $GITHUB_ENV
          echo "UTC_YMD=$(date -u +%Y%m%d)" >> $GITHUB_ENV
          echo "UTC_TS=$(date -u +'%Y-%m-%d %H:%M:%S')" >> $GITHUB_ENV

      - name: Run comments scraper
        id: run_scraper
        shell: bash
        run: |
          set -euo pipefail
          URLS_FILE="automation/comments_targets.txt"
          if [ -s "$URLS_FILE" ]; then
            URLS=$(tr -d '\r' < "$URLS_FILE" | paste -sd "," -)
            python -m automation.scrape_comments --url "$URLS" --outdir data --take 50 | tee run.log
          else
            echo "targets file not found: $URLS_FILE" >&2
            exit 1
          fi
          CSV="data/comments-${UTC_YMD}.csv"
          if [ -s "$CSV" ]; then
            ROWS=$(awk 'NR>1{c++} END{print c+0}' "$CSV")
          else
            ROWS=0
          fi
          echo "collected=$ROWS" >> "$GITHUB_OUTPUT"

      # ---------------- A13: keyword scan ----------------
      - name: "A13: keyword scan for comments + PR body & label"
        id: a13_scan
        if: ${{ success() }}
        shell: bash
        run: |
          set -eu
          CSV="data/comments-${UTC_YMD}.csv"
          KW="${A13_KEYWORDS:-レーザー,バグ,クラッシュ}"
          if [ ! -s "$CSV" ]; then
            echo "hits=0" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          hits=0
          # 先頭行はヘッダのためスキップ
          tail -n +2 "$CSV" | while IFS=, read -r cid url author content posted_at collected_at; do
            [ -z "${content:-}" ] && continue
            for k in $(echo "$KW" | tr ',' ' '); do
              case "$content" in *"$k"*) hits=$((hits+1)); break;; esac
            done
          done
          echo "hits=$hits" >> "$GITHUB_OUTPUT"
      # ---------------------------------------------------

      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v6
        with:
          commit-message: "A12: comments scrape ${{ env.UTC_DATE }} (UTC)"
          title: "A12: comments scrape ${{ env.UTC_DATE }}"
          body: |
            Automated run at (UTC): ${{ env.UTC_TS }}
            **collected = ${{ steps.run_scraper.outputs.collected }}**
            **keyword_hits = ${{ steps.a13_scan.outputs.hits }}**
            - Includes: data/comments-${{ env.UTC_YMD }}.csv
            <details><summary>log</summary>

            (see Actions logs)

            </details>
          branch: auto/a12/comments
          delete-branch: true
          labels: |
            automation
            A12

      - name: "A13: add labels when hits"
        if: ${{ success() && steps.a13_scan.outputs.hits != '0' }}
        uses: actions/github-script@v7
        with:
          script: |
            const {owner, repo} = context.repo;
            const prs = await github.rest.pulls.list({
              owner, repo, state: 'open',
              head: `${owner}:auto/a12/comments`,
              per_page: 1
            });
            if (prs.data.length){
              await github.rest.issues.addLabels({
                owner, repo, issue_number: prs.data[0].number,
                labels: ['A13','keyword-hit']
              });
            }
