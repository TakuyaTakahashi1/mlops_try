name: "A11: scheduled scrape & PR"

on:
  schedule:
    - cron: '0 0 * * *'  # UTC 00:00 = JST 09:00
  workflow_dispatch: {}

permissions:
  contents: write
  pull-requests: write

concurrency:
  group: a11-scrape
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install deps
        run: pip install -r requirements.txt

      - name: Create .env for script/tests
        run: |
          cat > .env <<'ENV'
          DB_URL=postgresql://example
          API_KEY=secret
          ENV

      - name: Run scraper
        run: python automation/scrape_titles.py

      - name: Upload CSV artifact (for debugging)
        uses: actions/upload-artifact@v4
        with:
          name: scraped-csv
          if-no-files-found: ignore
          path: |
            data/titles.csv
            data/daily/*.csv

      - name: Create PR with changes
        uses: peter-evans/create-pull-request@v6
        with:
          commit-message: "chore(data): daily scrape"
          branch: chore/daily-scrape
          title: "A11: daily scrape (data/titles.csv)"
          body: |
            ## Summary
            Run automation/scrape_titles.py daily (09:00 JST), commit results, and open a PR.

            ## Changes
            - Update data/daily/titles-YYYYMMDD.csv
            - Merge into data/titles.csv (dedup)

            ## Why
            Keep the dataset fresh automatically.

            ## How to verify
            - This PR includes updated CSVs with today's date.
            - CI checks are green.
          labels: data, automation
          delete-branch: true
